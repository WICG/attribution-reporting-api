# Attribution Reporting API

Sep 7, 2021

Meet link: [https://meet.google.com/jnn-rhxv-nsy](https://meet.google.com/jnn-rhxv-nsy)

Previous meetings: [https://github.com/WICG/conversion-measurement-api/tree/main/meetings](https://github.com/WICG/conversion-measurement-api/tree/main/meetings)

Use Google meet “Raise hand” for queuing.

If you can’t edit the doc during the meeting, try refreshing as permissions may have been updated after you loaded the page.


# Agenda

Add agenda topics here!



* Introductions
* Scribe volunteer
* Maud: new blogpost series “Privacy Sandbox progress”: [https://developer.chrome.com/tags/progress-in-the-privacy-sandbox/](https://developer.chrome.com/tags/progress-in-the-privacy-sandbox/) 
* Erik: update on aggregate design
    * [Aggregation via oblivious bucketization proposal](https://github.com/WICG/conversion-measurement-api/issues/206)


# Attendees — please sign yourself in! 



1. Charlie Harrison (Google CHrome)
2. Erik Anderson (Microsoft)
3. John Delaney (Google Chrome)
4. Alex Turner (Google Chrome)
5. Erik Taubeneck (Facebook)
6. Maud Nalpas (Google/Chrome)
7. Kim Laine (Microsoft)
8. Mariana Raykova (Google)
9. Melissa Chase (Microsoft)
10. Badih Ghazi (Google Research)
11. Joel Pfeiffer (Microsoft)
12. Aditya Desai (Google)
13. Chenkai Weng
14. Jonasz Pamuła (RTB House)
15. Betul Durak (Microsoft)


# Notes


## Maud: new blogpost series “Privacy Sandbox progress”



* Share a quick update
* Attribution Reporting API is part of the Privacy Sandbox project (umbrella name for project to add privacy preserving APIs for 3pc).
* New blog post series to make it easier to follow progress on new experiments / developments
    * Monthly
    * Subscribe to RSS feed
    * First edition is available!
    * [https://developer.chrome.com/tags/progress-in-the-privacy-sandbox/](https://developer.chrome.com/tags/progress-in-the-privacy-sandbox/)
    * Skewed to developers
* Should hopefully make life easier for Privacy Sandbox API updates


## Erik / Melissa: Update on aggregate design



* Erik: Filed issue on the conversion measurement API repo with link to oblivious bucketization proposal
* Asking for feedback on the API. Better for you or worse than the current MPC setup?
* Thanks to MSR folks for looking at other approaches to aggregate
* Melissa: high level approach, calling it “oblivious bucketization”
* Chenkai worked on this during his internship w/ Microsoft ads teams
* “Attribution Reporting API with Aggregate Reports” is a starting point
    * Want similar privacy guarantees
    * Output is DP
    * Provide comparable efficiency 
    * ALlow for more flexibility in record length
* System design: very similar to existing design
    * Browsers sending shares to two / three servers
    * Non-colluding / honest-but-curious
    * Reporting origin → queries are differentially private
        * Intermediate data is also DP for aggregation servers
* Big change
    * 3 servers are non-colluding
    * 3 parties can make protocols much more efficient than 2
* Record structure
    * Key k: string of bits with different segments corresponding to different features
    * List of values v1,...vm values you might want to aggregate
    * (k, v1,...,vm) will be secret shared to 2 servers
* 
* Jonasz: What is secret shared? (keys and values, or just values?)
* Melissa: Each bit string is split into two bits for keys
    * Values → integer shares
* Mariana
    * Multiple values associated with the same key? How are we aggregating?
* Melissa:
    * Browser just bundles all this information. Reporting origin can query adaptively / dynamically.
    * V1,...vm all coming from the same clients
* Phillipp: Shared between two servers, 2 out of 3?
* Melissa: only shared between two servers. Still assume honest majority. Regular xor sharing. Pulling 3rd server just for computation
* Bucketization
    * Input: a set of bit positions corresponding to features to subdivide on
    * “Bits 15-20 encodes ad campaign, group by those”
    * Output: Agg servers get a set of secret shared record for each possible value of that feature
    * Count → counts of items in each “bucket”
    * Sum → also adds up the value
        * Input: bucket number, value to add up
        * Output: Sum of corresponding values for all records in bucket
    * Subdivide, chop up buckets based on different set of bits
        * Input: bucket number, bits positions to subdivit
        * Repeat bucketization w/in selected bucket
* Mariana: All adaptively
* Melissa: Yes, different from current proposal
* Mariana: Can decide first to subdivide by?
* Melissa: Yeah, first take bits, 25-30 breakout based on
* Erik: Clarify, want to clarify. Vector of different values, in the current proposal this would just be a separate bucket.
* Melissa: Yep, that’s a difference. Not entirely sure how you’d do this in the current scheme. This proposal has one record has everything in one place.
* Erik: Sure, slightly different ways you’d handle the DP that gets added on, but you should be able to do the same thing with either.
* Melissa: Privacy goals of bucketization
    * Hide exact counts in each bucket (so you don’t reveal counts to agg servers)
    * Each server adds “dummy records” to each bucket. They have 0 values but just add random noise to each count
    * Hide which records are dummy records
    * Hide which record ends up in which bucket
    * Want: views of reporting origin and agg servers are all DP
* Achieving the privacy goals
    * 1. Add dummy records to the end. Look at each bucket. For each bucket, choose number T from truncated laplace. That’s how many dummies they will add. 0 for all the values. Add T records to that bucket
    * S1&S2 have shares of the original database. Dummies from both S1&S2 servers
    * Charlie: When we are adding the dummys, we have to add records proportional to the domain of all possible keys, the order of the number of dummys is the value T from the truncated laplace times the total domain?
    * MelissaSay you are bucketing based on 10 bits, you will need 10 times the the 1000
    * Charlie: Proportional to the domain size you are looking at dynamically
* Melissa: Secret sharing of original dataset, two extra sets of dummy records. Still not great since we know where the dummies are (at the end)
    * This is where we introduce the third party → shuffles everything together
    * S1 & S2 now have new shares. Some come from the old records, some from new dummy records
    * S1 & S2 reveal their shares of the bits that correspond to which bucket it goes in.
        * Combine shares of selected bits to reveal features and determine buckets
* Putting it all together
    * Secret shared records
    * Add dummy records
    * 3rd server shuffles
    * Reveal bits according to chosen feature → determines buckets
    * Can now add up values in buckets + noise, or subdivide and repeat the protocol
* Mariana: shuffling is not done only by the third server?
* Melissa: Yes it is a 3-party protocol to do the protocol
    * [Paper](https://eprint.iacr.org/2019/518.pdf)
    * Looking at set operations / unions / etc. Also includes a permutation protocol
    * Do the permutation twice, smoosh it together
* Mariana: Do you improve performance over two-party shuffle?
* Melissa: way more efficient since there is no crypto
* Mariana: Need to read the paper
* Melissa: Happy to chat afterwards if it helps. 3 party model, things get much easier
* Chenkai: “The protocol is adopted from the "Permute" protocol at page 13 of the paper https://eprint.iacr.org/2019/518.pdf “
* Melissa: there is a prototype implementation that all runs locally
    * Sampling DP noise, generating extra buckets, prgs to expand seeds, etc.
    * Doesn’t do sums right now
    * Communication wise: 2x size of record set gets passed around (with factor of L the size of the bucket space)
* Mariana: timings shown here. 128 bit key, what are you computing?
* Melissa: Filtering on 10 bits, 1024 buckets
* Jonasz: Dataset you shuffle. Do you shuffle with values or just the key?
* Melissa: Whole record. Full key and values concatenated
    * Need to be careful when you share. Some places you XOR, some you're adding over a field
* Jonasz: Concern with helper having access to initial dataset and shuffled data set. Can you identify dummy records?
* Melissa: No, because we _reshare_ the records. Rerandomizing them.
* Comparison with Incremental DPF approach
    * Privacy wise should be similar
    * Bucketization requires 3 non-colluding servers
    * Performance, DPF 2 orders of magnitude larger
    * DPF requires more computation*
        * Mariana/Melissa: &lt;missed in notes>
        * Mariana: Did you run DPF with 1024 buckets? For comparison I think 1024 is quite small
        * Melissa: Multiple bucketization approach I think will be more efficient. Didn’t actually compare with similar buckets though
    * Query flexibility
        * DPF can only evaluate one hierarchy of histograms, only choice is how long a prefix to use
        * Bucketization can build a hierarchy of histogram using any subsets of key bits at each level
        * DPF computes only sums, bucketization can choose which aggregates to compute on the fly. Could do any MPC computation, since each value is secret shared
            * Mariana: DPFs also gives you secret shares if you expand all the way down
            * Melissa: assuming you wouldn’t expand all the way down, but possible
            * Mariana: if you know exactly which keys you are interested in, DPFs you can directly evaluate for those
* Mariana: What if you go to 2 parties? Does it scale?
    * Melissa: Way more expensive w/ 2 parties. Nlogn OTs (see [Cryptology ePrint Archive: Report 2019/1340 - Secret Shared Shuffle (iacr.org)](https://eprint.iacr.org/2019/1340))
    * Mariana: Do you have actual numbers?
    * Melissa: We have microbenchmarks, but didn’t implement the system
    * You need a sorting network
* Charlie: Most concerned about the requirement to add dummy records proportional to the size of the record space. Explored dpfs because the domain space is so large and sparse. I’d be curious to look at a head to head between dpf and this with some dynamic expansion. Otherwise, I agree these give a more flexible system.
* Melissa: Particularly if you are doing the dpf approach where th servers look at each sets of buckets to compute the counts at each level, you are revealing more information. With oblv bucketization it is a much wider tree. Different routes to use each one
* Charlie: If you are doing a hierarchical approach in this design, you will need to do something similar where you have dp counts to the helpers to do some pruning.
* Melissa: If we take 10 levels of the dpf tree and add dummy records, then throw out empty buckets and repeat. The dpf tree you can expand with computing the values at all...privacy should be the same but efficiency is hard to compare
* Charlie: Stepping away from DPF, comparing this to Prio, this is giving you communication costs on a per record basis because you don’t need to encode the entire domain in each record. For something like 10 bits it’s probably not a big deal, but for a complicated hierarchy it would be worse
* Erik T: Wanted to make a comment. Understanding from prev protocol was that it was a malicious protocol. 
* Melissa: don’t remember the assumptions in the other one
* Mariana: Malicious with respect to privacy, a single malicious server
* Melissa: We think it might be possible don’t have a solution at the moment
* Erik T: Is malicious a hard requirement? These things come in stages.
* Charlie: Malicious security is ideal but not a hard requirement
* Erik A: We are also thinking through threat models. Generally OK with honest-but-curious but prefer stronger guarantees
* Chenkai: Using secret share MPC, still possibilities to make it maliciously secure with extra cost. 3 party setting allows at most one bad guy. Still have to rely on assumption (honest majority) but don’t allow any corruption
* Melissa: require at least 2 of the parties following protocol and one rogue
* Mariana: Strong views on 2 vs 3 vs 4?
* Charlie: Not just a question for browsers, it is going to depend on who is operating these servers, and the operational/maintenance challenges for supporting these systems. Whether 3 vs 2 vs 4 is a big difference is not clear, would be great to get feedback from folks who have any experience running these systems whether 3_ is very different would be helpful.
* Erik T: I can say that 2 is difficult. The number of bilateral relationships you need to maintain increases factorially. 3 might be doable 10 is impossible. If a 3 party protocol gives a major benefit to efficiency, it will work itself out because it will be cheaper.
* Erik A: Seems reasonable that a cheaper protocol will allow for more players, etc.