# Attribution Reporting API

July 26, 2021

Meet link: [https://meet.google.com/jnn-rhxv-nsy](https://meet.google.com/jnn-rhxv-nsy)

Previous meetings: [https://github.com/WICG/conversion-measurement-api/tree/main/meetings](https://github.com/WICG/conversion-measurement-api/tree/main/meetings)

Use Google meet “Raise hand” for queuing


# Agenda



* Introductions
* Scribe volunteer
* MaskedLark proposal ([https://github.com/WICG/privacy-preserving-ads/blob/main/MaskedLARK.md](https://github.com/WICG/privacy-preserving-ads/blob/main/MaskedLARK.md))
    * [Slides](https://github.com/WICG/privacy-preserving-ads/blob/main/presentations/20210726%20-%20Masked%20LARK%20presentation.pdf)
* Any other business


# Attendees — please sign yourself in! 



1. Charlie Harrison (Google Chrome)
2. Denis Charles, Microsoft
3. Erik Anderson (Microsoft)
4. Andrew Knox (Facebook)
5. Basile Leparmentier (Criteo)
6. Brendan Riordan-Butterworth (IAB Tech Lab / eyeo GmbH)
7. Antoine Bisch (Google)
8. Andrew Pascoe (NextRoll)
9. Valentino Volonghi (NextRoll)
10. Larissa Licha (NextRoll)
11. Erik Taubeneck (Facebook)
12. Mariana Raykova (Google)
13. Bill Landers (Xandr)
14. Abdellah Lamrani Alaoui (Scibids)
15. Jonasz Pamuła (RTB House)
16. Badih Ghazi (Google)
17. Karn Seth (Google)
18. Paul Marcilhacy (Criteo)
19. Chris Evans (NextRoll)
20. Przemyslaw Iwanczak (RTB House)
21. Aditya Desai (Google)
22. Daniel Rojas (Google)


# Notes



* Scribe - Brendan Riordan-Butterworth

Charlie Harrison: Microsoft is going to go over MaskedLark proposal.  Then there might be time to go over additional items.  Add them to the agenda if you’d like.  Give the floor over to the Microsoft presenter.  

[Slides](https://github.com/WICG/privacy-preserving-ads/blob/main/presentations/20210726%20-%20Masked%20LARK%20presentation.pdf) 

Denis Charles (Charles): Masked LARK is a proposal for doing conversion reporting and modelling, building on other ideas that people have.  This is from Microsoft.  I think this (conversion measurement) is a noble endeaver, and expect that there is a lot of work.  

Charles: Goals of the talk: Introduce technical aspects of Masked LARK, the aim, the algo, tech details, understand how it differs, recognize the known limitations.  

Charles: No API or workflow discussion.  We can cover this later.  Focus on the ideas here, so that you can see the challenges and etc.  

Charles: Background: Third party cookies are going away, this has a bunch of implications, especially fraud detection, targeting, and conversion reporting and modelling.  This talk focuses on the last.  

Charles: For a complex space, we are starting with a baseline of a bunch of existing proposals, so we’re looking at how to increase the scope and utility of existing ideas.  

Charles: Look at the trusted mediator for the abstraction for all parties that need to trust for a transaction.  Google’s proposal does this without trusting a single party, uses a variety of partially trusted parties.  This helps solve the complex problem of how do we handle the trust.  

Charles: MPC handles the above.  

Charles: We see multiple parties as more palatable, in this proposal.  Expect that every intermediary be always available and willing to process.  The Google proposal handles reporting, but less modelling.  

Charles: The Google proposal might be more complex that necessary - we want do design with simple ideas for privacy, tho not with the effect of losing the feature.  

Charles: One part of our thinking was starting at what the set of users is - from the ad network, helpers, consumers, people in ad network trying to reason about system that is being built.  How do we meet with these people in places that they’re comfortable with?  

Charles: Proposal - Differentially private Map-Reduce framework?  This is very much like a map-reduce, but it does add a flavor of privacy at each reduce operation.  Any MPC function that is differentially private can be a reduce operation, but focus at the moment is using Sum only.  

Charles: For Helpers, the reqs are very similar to secure aggregation, where many things can be done within 2 rounds (map/reduce), but some modelling might need multiple map/reduce rounds.  Helpers compute and output a vector.  

Charles: gets into the math as part of the presentation.  (Slide 7, link presentation?) The Ad Network will publicly disclose some aspects (epsilon and K).  

Charles: Compare with Google proposal, there is no explicit hiding of the aggregation keys.  

Charles: Examples: Counting conversions.  If less than “N” users, and have 2 helpers, then there’s some additional math that gets into how to count conversions (Slide 8 from presentation).  No explicit enforcement of granularity.  

Charles: The keys are hidden from the helpers in a proposal.  

Charles: Model training.  The pain point for model training is bringing together a whole lot of data into one place, which has privacy implications.  But what we want to get is a local feature vector.  The feature values can then be aggregated.  However, computing the vector at the user’s device implies federated learning, and then there’s unpredictable cost on user’s device.  Federated Learning is probably not that realistic.  But if the user trusts the helper with label, then computation can be determined at the server (multiple helpers preferred).  

Charles: We were stuck here for a while, but came up with a new idea, Masked Aggregation.  What we can do is (imagining binary labels), is to have the browser send both possible labels to the helper, locally compute alphas and betas according to specific rules, and send the alphas and betas to Helpers, but specific.  This is discussed further in Slide 10.  

Charles: working from this kernel, we can build new features on top.  

Charles: There are a lot of known issues that can come from this, with potential solutions, discussed further on Slide 12. 

Andrew Knox: With the k-anonimity guarantees?  Is diff privacy enforced strictly by the browser, or by the helpers?

Charles: Yes, the browser can inject local noise, but the helper should be adding in.  

CH: For the k-anon, we are counting the number of contribs per key.  How are we doing this?  

Charles: We kindof still like this particular property of k-anon even though it can be easily attacked by adding fake reports for any key.  The attacks are probability limited because they would need to know what keys, but it’s easy for the consumer to understand.  

CH: Yes, the question is about whether the helper potentially knowing the pattern of clients.  Doing a distinct count seems to open another identification vector.  

Charles: Yes, that is a concern.  

Mehul Parsana: When we are trying to make a masked proposal, if there is a user ID, then there are still multiple records associated with that ID.  We could possibly mask the user ID, hiding it from further down the line.  Also, if they were temporally unique, then you could make it unavailable.  But this affects dynamic rolling interval reporting.  

CH: One concrete way I could see this being abused / a leak: imagine logging imps on 2 sites, and the agg key is a public value, not being protected, but we do have a pseudonumous ID for the user, then we could evaluate the pattern of users across the sites, even if there isn’t a 1:1.  If you segmented user IDs and said they must be public, then there might not be the issues.  

Charles: Found necessary slide.  Known issues and potential solutions.  This is a building block.  We are not actually hiding agg keys, but they are opaque.  

Charles: K-anonyminity can be attacked by ballot stuffing, model training has vulnerabilities as well.  Detailed in Slide 12.  Discoverable feature vector, leaking label space in non-binary cases, timing attacks, and solution is limited to models that are continuous functions.  

MP: If you tried to hide a user ID, it becomes an expensive process (MPC is expective).  

Jonasz Pamula: Quick comment regarding your concern about counting different users with diff keys.  It might be acceptable to send user ID module, like “1000”, instead of counting unique users with a given key.  

Charles: Thanks, sounds like a good suggestion.

Mariana Raykova: I wanted to make a point that if you want to hide model features, you could upgrade model.  I like the oblivious design!  Using a simple multiplication, then using the helpers to calculate the gradients.  You might not be that far from the private model.  

MP: This may have come up, but it would still be expensive for the two-party.  

MR: Calculation can be done offline.  But the third party (or the two parties), someone knows all the impression data.  Even if it’s not the involved party, you can precompute and enable that the helpers can do a lookup instead of a function.  

MP: This is a good idea, we have a few other things we need to get to before.  

CH: Concern, request for feedback.  This is based on the label space, and the granularity we can achieve.  In the current proposal (not this one), multiple sites can mix, but then we need to hide the key.  If the key associates with one site (this proposal), then we have to do all things per site, which has implications on the usability of the data.  In this proposal, I don’t see a neat way to report on product categories or shopping cart contents.  These would require shipping around humungous vectors with each impression. Not necessarily a deal breaker, but a concern.  Flagging it, is this a concern?  

MP: Hasn’t come up clearly, but let’s make sure it’s in GitHUb.  

CH: Yes, [here as Issue](https://github.com/WICG/privacy-preserving-ads/issues/23) 23

MP: Cool, we’re thinking about it as pretty sparse.  Might not be a big challenge, but worth looking through further.  

Erik Taubeneck: To CH comment, questions/comment.  (1) I’m not convinced that the only reason we need to hide the keys is the data, the pub to single ad might need to be hidden, especially if the pub is running a helper.  (2) With conversion side values, we had talked about having a declarative syntax instead of full JS env.  The same declared syntax might be usable here.  This could improve the MPC.  

MP: Yes, more info on (2) please.  With regards to (1), that’s where fake reports come in that get cleared out over aggregation.  But the overhead and efficacy of this will depend.  

ET: I think this is a bigger threat that just 1 reason (to keep keys private).  

CH: Yes, there is also timing attack.  What does a revealed key have on the helper?  Ultimately, it should be nothing.  This introduces another problems: one of the reasons / benefits for the current design is snappier reporting.  The delay gets more necessary with today’s proposal.  This proposal seems to emphasize the modelling over the reporting.  

Andrew Knox: How much about poisoning?  A malicious helper?  

Charles: Yes, definitely.  The user side there are thoughts.  

CH: There is an issue on this one, too.  Input validation will probably require communication between helpers.  

ET: Why the diff privacy on every reduce step instead of just the final?

Charles: Caution.  Probably should only be the last when there’s muti-round, or at least less. 

MR: It isn’t yet obvious how to calculate risk.  So depending on the model, we might need to add each round.  

CH: How can we establish proof of DP?  We should keep it an active topic, but it’s like P != NP
