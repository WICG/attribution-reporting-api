# Attribution Reporting API

Aug 9, 2021

Meet link: [https://meet.google.com/jnn-rhxv-nsy](https://meet.google.com/jnn-rhxv-nsy)

Previous meetings: [https://github.com/WICG/conversion-measurement-api/tree/main/meetings](https://github.com/WICG/conversion-measurement-api/tree/main/meetings)

Use Google meet “Raise hand” for queuing.

If you can’t edit the doc during the meeting, trying refreshing as permissions may have been updated after you loaded the page.


# Agenda



* Introductions
* Scribe volunteer: Charlie Harrison
* Agenda:
    * [#194](https://github.com/WICG/conversion-measurement-api/issues/194), Declarative API for Aggregate Reports
    * Encoding choice for aggregate payloads (JSON, CBOR,...), [#195](https://github.com/WICG/conversion-measurement-api/issues/195)
* Any other business
    * [#185](https://github.com/WICG/conversion-measurement-api/issues/185): Inclusion of time stamp in conversion measurement report


# Attendees — please sign yourself in! 



1. Brian May (dstillery)
2. Erik Taubeneck (Facebook)
3. Charlie Harrison (Google Chrome)
4. Brad Lassey (Google Chrome)
5. John Delaney (Google Chrome)
6. Alex Turner (Google Chrome)
7. Benjamin Case (Facebook)
8. Brendan Riordan-Butterworth (IAB Tech Lab / eyeo GmbH)
9. Moshe Lehrer - Neustar
10. Raj Gupta (Captify)
11. Bill Landers (Xandr)
12. Larissa Licha (NextRoll)
13. Vishnu Prasad (Nielsen)
14. Badih Ghazi (Google Research)
15. Andrew Pascoe (NextRoll)
16. Valentino Volonghi (NextRoll)
17. Jeremy Ney (Google)
18. Maud Nalpas (Google Chrome)
19. Basile Leparmentier (criteo)
20. Aloïs Bissuel (Criteo)
21. John Lawrence (Nielsen) 
22. Aditya Desai (Google)
23. Daniel Rojas (Google)


# Notes


## [#194](https://github.com/WICG/conversion-measurement-api/issues/194), Declarative API for Aggregate Reports



* John: 
    * explainer has considerations for a different mechanism for generating input. Current design has a requirement that input generated in worklet, however lots of places where conversions registered just with HTTP pings, no JS on the page
    * What is the feature parity between declarative API and worklet
    * Worklet
        * Buckets can be a function of both impression-side and conversion-side info
        * Value also
        * You also have an ability to filter conversions based on source-side information
        * Simple to implement → just add more information as arguments to the JS worklet
    * We need to decide the subset of features in a declarative API
    * Simplest: impression declares “half” the histogram bucket, conversion declares the “other half”.
        * Loses the generality of the worklet solution
        * Possibly, declarative buckets will not be compatible with JS-based buckets
        * [Issue 176](https://github.com/WICG/conversion-measurement-api/issues/176) → adding more capabilities to the worklet (seeing remaining privacy budget). Would be difficult to add feature parity there.
        * Custom attribution models based on previous attribution sources (whole user journey)
    * No matter what, there will be some differences in the capabilities, possibly a big issue
* Erik T:
    * Thanks for writing down the issue
    * Maybe there is a simple model that is a bit more complex that might get us further
    * Source-side denotes the buckets that they want to specify
    * Trigger-side does the same with the values
    * Do an intersection
    * Source: 5,6,7
    * Trigger: 5, 7
    * Final → 5,7
* John:
    * Does give some of the dynamic capability
    * One step further → boolean matching at bucket level
    * At source side, bucket five has a flag set, and on the event side I can have some set of flags to try to match them up
* Brian May:
    * Like this idea a lot too
    * How do you know it is actually working?
    * E.g. that the source-side and destination side is being accurately merged
    * If can’t answer that question, can’t trust it
* John:
    * Speaks to a greater problem. How do you know the worklet is doing what it’s supposed to be doing?
    * Is that what you’re talking about?
    * Brian: Yes
* Erik T:
    * Local test environment solve this?
* Brian:
    * Perhaps
* John:
    * Making sure that the browsers can expose what’s happening for local testing
    * Beyond local testing is still an open question
    * Intersection idea: worth noting that that couldn’t support things like
        * Privacy budgets
        * Maybe multiple attribution models, lots of complexity
* Erik T:
    * Haven’t thought a ton about the privacy budgeting
    * Attribution model set by the browser, don’t see it as being in conflict
* John:
    * Idea is that there could be a follow-up where different attribution models could be
* Charlie:
    * Concern with complexity of information that needs to flow to client, combinatorial explosion of all possible keys
* Erik T:
    * On the source side, you could handle that with a 32 bit string, have a 1 for each of the buckets you want to consider
    * On trigger side, you need hash map of 2^32, look at the intersection
* Charlie:
    * I think it would be a lot of communication even on the source-side
* Erik:
    * Yes would get fairly big. Maybe some compression could solve
    * Complex on the server side, but possibly nice, have a lot of flexibility
* John
    * 1000 products use-case is a worst-case model
    * Intersection solution allows you to do higher level things
    * Once we determine the feature set it will be easier
* Erik T:
    * Over engineering a bit maybe but
    * Allow for different ways to chop up the bits
    * First 8 bits as campaign ID, remaining bits as product ID
* Brian May:
    * Sounding like there is a solution to a subset of use-cases that’s worth exploring
    * Perhaps someone could come up with a few and test this idea against them
* John D:
    * If we have some sample mechanisms documented on the issue we can stress test them
* Erik T:
    * One of the original reasons this was opened was that you may not have JS access on the page. Because this is in a worklet, do you need JS access on the page
    * Couldn’t the trigger event just specify the location of the JS and have the browser run it
* John D:
    * Technically this is possible
    * This runs into a bunch of issues with the security model 
* Charlie:
    * Lots of existing security features will need to be updated
* Erik T:
    * If you could declare a very simple SQL statement (maybe other DSL)
    * A few simple functions (group by, join, max, etc)
    * Maybe in the limit this is the same generally as JS
* John:
    * Some other proposals like Shared Storage fits a bit better
* Charlie:
    * Could have some DSL that isn’t JS and doesn’t need to fit the requirements of putting JS on the page. IF I give you some simple SQL statement, can you prove you can’t do Spector or some other XSS attack?
    * I think this is something we could consider, even for the worklet model. You could imagine that not being a JS model at all, but a processing environment of some other DSL that isn’t Turning complete, or safer than JS.
    * Somewhat dangerous, difficult to do the security analysis.
* John:
    * Introducing a new model is difficult, easier if it’s modeled by the browser.


## Encoding choice for aggregate payloads (JSON, CBOR,...), [#195](https://github.com/WICG/conversion-measurement-api/issues/195)



* John:
    * Filed by Alex
    * How concerned with performance should we be with sending aggregate reports
    * Current explainer: reports encoded with JSON, millisecond timestamps will need to be encoded as strings, etc.
    * Helper payloads are encrypted, but JSON doesn’t support binary, needs to base64 encode them, adds some overhead
    * Could use CBOR (concise binary object representation)
        * Allows storing 64 bit intergers
        * Binary data like encrypted data is directly supported
    * Alternatively, could do things like gzip the JSON reports
    * JSON is super common on the web, while something like CBOR isn’t
        * Can see exactly what’s in JSON, human readable
    * CBOR will require different tooling, libraries, etc.
    * Developer-wise there are issues with CBOR
    * Shared data
        * Scheduled report time
        * Privacy budget key
        * All these things need to be sent both in the encrypted payload and to the reporting endpoint
        * We could use authenticated encryption with associated data (AEAD)
        * Only send the data once in the report
* Brian May:
    * I would start with the thing that’s most accessible to the developer, and explore other solutions later
* Charlie:
    * One benefit of using a binary encoding is that we’ll be able to remove a possible class of bugs. 
    * In the current design there is a bunch of data that is replicated 3 times in the report. 
    * There is data sent to the reporting origin as well as both helpers. 
    * With AEAD, it only has to be generated once, when you get the report. 
    * It makes the payloads more like what the aggregation service needs to read. 
    * Beyond the bandwidth saving, this makes it easier to understand and more concise.
* John:
    * One other thing from someone who’s not as familiar with how AEAD works
    * All data the helper needs to interpret is encrypted to their key
    * The parallel data model complicates things
* Erik T:
    * First instinct is to agree with Brian
    * For an Origin Trial, let’s keep it simple
    * Do agree with Charlie’s points that it gets rid of a whole class of bugs
    * Should be able to check the data either way (https), so we’re not totally flying blind
    * Simple first is probably the right way to start
* John:
    * We could do simple for the ad-tech, but we still need to decide what the helper sees (encoding of the encrypted payload)
    * Helper servers maybe a different class of developers
* Brian May:
    * Great point, two different kinds of recipients, latter is going to be much more capable of getting the kind of attention it needs to operate properly
* Erik T:
    * We don’t have a lot written down on helper servers yet, but
    * I imagine the helper servers are running some sort of preset open source docket container or something
    * The company that is running the service probably has very little to do to actually deal with the encdings
    * The function they need to run should be determined by the output that the browser is providing
    * We should do whatever makes sense for the things in transit
    * Nothing needs to be done with it, it’s an end to end thing written by this group here.
* Charlie
    * That’s one way it could go.
    * Not trying to prescribe yet where the helper servers will go.
    * There is a world where helper servers are running what’s written.
    * Another compatible world where helper servers are running their own implementation of their protocol.
        * From a security point of view, if you have two implementations, if one has a bug, at least you have the other one. 
        * For an origin trial, you might just have one.
        * For the long run we wouldn’t preclude each running their own implementation.
* Brian May:
    * Agree with those two possible futures
    * Second one (multiple implementations) may be more likely
    * But the number of instances of the services will be significantly smaller
    * Better to focus on a more concise and reliable messaging system than JSON
* Maud N:
    * Asking some web developers whether they use CBOR before and it doesn’t look super common
    * Makes barrier to adoption higher


## [#185](https://github.com/WICG/conversion-measurement-api/issues/185): Inclusion of time stamp in conversion measurement report



* Vishnu Prasad:
    * The issue had a number of different questions. Top of mind
    * Right now in the conversion reports, we don’t have the timestamp information within it
    * Exposure information is there, but timestamp is not
    * From previous response on the issue, we don’t have a philosophical disagreement with having time stamp at touch point level
    * 64 bits of information is there, why don’t we put the timestamp in there? Can we send it also in the conversion report
    * If we could get the timestamp (of the exposures) directly in the conversion report
    * Thoughts on this?
* Charlie
    * In the design we thought of the impression id as a lookup key in a table which could encode any auxiliary information
    * Rather than showing in the report, you could look this up in the logs
* Vishnu
    * Challenge, how do we get the timestamp information itself
    * So many DSPs and publishers, working with the brand
    * Timestamp is a dynamic piece of information, how is it passed on in a usable manner, across all the different vendors, dsps
* John Lawrence: from measurement POV, recency of the ad becomes important, from a cross channel measurement standpoint. Multiple ad media interacting. Recency becomes an important aspect. Who is driving which conversions
* John Lawrence:
    * When you say exposure, placing an ad on a DSP, still is possible
    * If third party tracking is still possible, unaffected
* Charlie
    * The 3rd party tracking use case is very effected.
    * We would need to think more deeply about this, how multi party reporting work is still up in the air.
* John L:
    * Can you clarify what you mean by exposure?
* Charlie
    * Using that as a impression
* John L:
    * Yes, so you don’t have user level data. You have impression data. We do this either from Google Cloud, or we put a companion tag. We can condense this to more specific needs.
* Charlie:
    * I think that supporting these flows is something we can explore further. I don’t think that adding a timestamp is necessarily going to cut it. I’d like to take a step back and look at this more holistically.
* Mehul Parsana:
    * 64 bit key, it would be helpful to have a bit of a diagram to see the information flow
    * From the edge POV, we converted into key value pair rather than creating the fixed bit vector
        * Mapping the 64 bit to the actual metadata is hard
    * Second question
        * Could be 128 bit ID
* John D:
    * We can produce some diagram
    * Maud has some that she has worked on and published
    * We expect there to be two lookups
        * Generate a key on the click side
        * At report time you look up the impression id and fills out the metadata
* Mehul
    * Lots of different ways the bit representation could change
    * Meaning changes
    * Complexity of tracking the entire mechanism is growing
    * How to maintain the dictionary would be useful, versioning
* Erik:
    * Understanding is you just generate a UUID for each individual impression
* Charlie:
    * This 64 bit ID is related to the event level API, and is distinct from the one use from the aggregate API.
* Mehul:
    * Ahh, I see.
* Charlie:
    * Right now, we do have these two types of reports. For the event level, there’s no aggregation, you have the ID in the clear.